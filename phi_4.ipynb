{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dabeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU flash_attn==2.7.4.post1\n",
    "!pip install -qU torch==2.6.0\n",
    "!pip install -qU transformers==4.48.2\n",
    "!pip install -qU accelerate==1.3.0\n",
    "!pip install -qU soundfile==0.13.1\n",
    "!pip install -qU pillow==11.1.0\n",
    "!pip install -qU scipy==1.15.2\n",
    "!pip install -qU torchvision==0.21.0\n",
    "!pip install -qU backoff==2.2.1\n",
    "!pip install -qU peft==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41572fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- processing_phi4mm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- configuration_phi4mm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- vision_siglip_navit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- speech_conformer_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- modeling_phi4mm.py\n",
      "- vision_siglip_navit.py\n",
      "- speech_conformer_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00,  4.91it/s]\n",
      "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/a11e830f953efbce02a57d3b672f84c5140e3f94/speech_conformer_encoder.py:2775: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:32<00:00, 10.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import soundfile as sf\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# Define model path\n",
    "model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='flash_attention_2',\n",
    ").cuda()\n",
    "\n",
    "# Load generation config\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec349d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac2f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt structure\n",
    "user_prompt = '<|user|>'\n",
    "assistant_prompt = '<|assistant|>'\n",
    "prompt_suffix = '<|end|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bbeb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- IMAGE PROCESSING ---\n",
      ">>> Prompt\n",
      "<|user|><|image_1|>What is shown in this image?<|end|><|assistant|>\n",
      ">>> Response\n",
      "A stop sign in front of a building with Chinese writing on it.\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Image Processing\n",
    "print(\"\\n--- IMAGE PROCESSING ---\")\n",
    "image_url = 'https://www.ilankelman.org/stopsigns/australia.jpg'\n",
    "prompt = f'{user_prompt}<|image_1|>What is shown in this image?{prompt_suffix}{assistant_prompt}'\n",
    "print(f'>>> Prompt\\n{prompt}')\n",
    "\n",
    "# Download and open image\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "inputs = processor(text=prompt, images=image, return_tensors='pt').to('cuda:0')\n",
    "\n",
    "# Generate response\n",
    "generate_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1000,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(f'>>> Response\\n{response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c8db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AUDIO PROCESSING ---\n",
      ">>> Prompt\n",
      "<|user|><|audio_1|>Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.<|end|><|assistant|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Response\n",
      "What we do as a society, we have to think about where we're moving to. I frequently talk to students about cognitive enhancing drugs and a lot of students take them for studying and exams, but other students feel angry about this. They feel those students are cheating and we have no long-term health and safety studies in healthy people and we really need those before people start taking them. <sep> Ce que nous faisons en tant que société, nous devons penser à où nous allons. Je parle fréquemment avec des étudiants sur les médicaments cognitifs et beaucoup d'étudiants les prennent pour étudier et les examens, mais d'autres étudiants se sentent en colère à ce sujet. Ils sentent que ces étudiants trichent et nous n'avons pas d'études de santé et de sécurité à long terme sur des personnes saines et nous en avons vraiment besoin avant que les gens ne commencent à les prendre.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 2: Audio Processing\n",
    "print(\"\\n--- AUDIO PROCESSING ---\")\n",
    "audio_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac\"\n",
    "\n",
    "speech_prompt = \"Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\"\n",
    "\n",
    "prompt = f'{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}'\n",
    "print(f'>>> Prompt\\n{prompt}')\n",
    "\n",
    "# Downlowd and open audio file\n",
    "audio, samplerate = sf.read(io.BytesIO(urlopen(audio_url).read()))\n",
    "# audio, samplerate = sf.read('audio.wav')\n",
    "\n",
    "\n",
    "# Process with the model\n",
    "inputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors='pt').to('cuda:0')\n",
    "\n",
    "generate_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1000,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(f'>>> Response\\n{response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#体はきっと拙の思いを理解してくれる。どうか正しいところに栄養が吸収されますように。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104273e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13c7b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Prompt\n",
      "<|user|><|audio_1|>Please transcribe the following audio content and give the <emotion> tag at the end. Emotions are divided into 8 categories: <sad>, <anger>, <neutral>, <happy>, <surprise>, <fear>, <disgust>, and <other>.<|end|><|assistant|>\n",
      ">>> Response\n",
      "体はきっと節の思いを理解してくれる。どうか正しいところに栄養が吸収されますように。 <sad> The body will surely understand the intention of the section. May the nutrients be absorbed in the right place.\n"
     ]
    }
   ],
   "source": [
    "speech_prompt2 = \"Please transcribe the following audio content and give the <emotion> tag at the end. Emotions are divided into 8 categories: <sad>, <anger>, <neutral>, <happy>, <surprise>, <fear>, <disgust>, and <other>.\"\n",
    "\n",
    "prompt = f'{user_prompt}<|audio_1|>{speech_prompt2}{prompt_suffix}{assistant_prompt}'\n",
    "print(f'>>> Prompt\\n{prompt}')\n",
    "\n",
    "# Process with the model\n",
    "inputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors='pt').to('cuda:0')\n",
    "\n",
    "generate_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1000,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(f'>>> Response\\n{response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第一個版本是vanilla --> 最後是<e>\n",
    "## 第二個版本是 *sad* ... --> 出來也是<e>\n",
    "## 第三個版本是 <sad> --> 就變 <sad>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
